---
title: "PSTAT 131 - HW2"
output: html_document
author: "Sarah Salem"
---
###Number 1:

Given functions

```{r}
library(readr)
library(ISLR) 
library(ggplot2) 
library(reshape2) 
library(plyr) 
library(dplyr) 
library(class)

spam <- read_table2("spambase.tab", guess_max=2000)

spam <- spam %>%
  mutate(y=factor(y, levels = c(0,1), labels = c('good', 'spam'))) %>%
  mutate_at(.vars=vars(-y),.funs=scale)
spam

#create error rate function
calc_error_rate <- function(predicted.value, true.value){
return(mean(true.value!=predicted.value))
}

records = matrix(NA, nrow=3, ncol=2)
colnames(records) <- c("train.error","test.error")
rownames(records) <- c("knn","tree","logistic")

set.seed(1)
test.indices = sample(1:nrow(spam), 1000)
spam.train=spam[-test.indices,]
spam.test=spam[test.indices,]

nfold = 10
set.seed(1)
folds = seq.int(nrow(spam.train)) %>% ## sequential obs ids
  cut(breaks = nfold, labels=FALSE) %>% ## sequential fold ids
  sample() ## random fold ids
head(folds)

do.chunk <- function(chunkid, folddef, Xdat, Ydat, k) {
  
train = (folddef!=chunkid)

Xtr = Xdat[train,]
Ytr = Ydat[train]

Xvl = Xdat[!train,]
Yvl = Ydat[!train]

## get classifications for current training chunks
predYtr = knn(train = Xtr, test = Xtr, cl = Ytr, k = k)

## get classifications for current test chunk
predYvl = knn(train = Xtr, test = Xvl, cl = Ytr, k = k)

data.frame(train.error = calc_error_rate(predYtr, Ytr), val.error = calc_error_rate(predYvl, Yvl))
}
```

Use 10-fold cross validation to select the best number of neighbors best.kfold out of six values of 
k in kvec = c(1, seq(10, 50, length.out=5)). Use the folds defined above and use the following do.chunk definition in your code. 
Again put set.seed(1) before your code. What value of k leads to the smallest estimated test error?

we can observe that validation/ test error is smallest when k =10 at 0.1005255

```{r}
set.seed(1)
kvec = c(1, seq(10, 50, length.out=5))
error.folds = NULL

YTrain = spam.train$y
XTrain = spam.train %>% select(-y)

# Loop through different number of neighbors
for (j in kvec) { # Loop through different number of neighbors # Apply do.chunk() function to each fold
  
  tmp = ldply(1:nfold, do.chunk, folddef = folds, Xdat= XTrain, Ydat = YTrain, k=j) %>% # Necessary arguments to be passed into do.chunk
    summarise_all(funs(mean)) %>%
    mutate(neighbors=j)
error.folds = rbind(error.folds,tmp)
}
error.folds
best.kfold = error.folds$neighbors[which.min(error.folds$val.error)]
best.kfold
```

###Number 2: Training and Test Errors

Now that the best number of neighbors has been determined, compute the training error using spam.train 
and test error using spam.test or the k = best.kfold. Use the function calc_error_rate() to get the errors from 
the predicted class labels. Fill in the first row of records with the train and test error from the knn fit.

```{r}
#Get predictions for training set
predYtr=knn(train=spam.train[,1:57],test=spam.train[,1:57], cl=spam.train$y,k=best.kfold)

predYvl=knn(train=spam.train[,1:57],test=spam.test[,1:57], cl=spam.train$y,k=best.kfold)

records[1,1]=calc_error_rate(predicted.value =predYtr,true.value =  spam.train$y) 
records[1,2]=calc_error_rate(predicted.value =  predYvl,true.value =  spam.test$y) 
records
```

###Number 3: Controlling Decision Tree Construction

184 nodes, 48 out of 3601 observations are missclassified.

```{r}
library(tree)
spamtree= tree(y~.-y,data=spam.train, control=tree.control(nrow(spam.train),minsize=5,mindev =  1e-5))
summary(spamtree)
```

###Number 4: Decision Tree Pruning

```{r}
library(maptree)
prune = prune.tree(spamtree, best=10) 
draw.tree(prune, cex=0.5, pch=0.5, nodeinfo=TRUE)

title("Classification Tree Built on Training Set")
```

###Number 5: CV

The optimal tree size that minimizes misclassification is 19

```{r}
set.seed(1)

cvtree = cv.tree(spamtree, method="misclass", K=10)
cvtree

best.size.cv = min(cvtree$size[which(cvtree$dev==min(cvtree$dev))]) 
best.size.cv

plot(cvtree)
abline(v = best.size.cv,col="blue")
 
```

###Number 6: Training and Test Errors

```{r}
#Prune the tree with best.size.cv
spamtreepruned = prune.tree(spamtree, best=best.size.cv) 
treetrain = spam.train[,1:(ncol(spam.train)-1)]
treetest = spam.test[,1:(ncol(spam.test)-1)]

#Get predictions & erros
predict_trainset = predict(spamtreepruned, treetrain, type="class") 
error_trainset = calc_error_rate(predicted.value = spam.train$y,true.value =  predict_trainset)

predict_testset = predict(spamtreepruned, treetest, type="class") 
error_testset = calc_error_rate(predicted.value = spam.train$y,true.value =  predict_trainset)

records[2,] = c(error_trainset, error_testset) 
records

```
###Number 7:

####Part A:

$p(z) = e^z/(1+e^z)$

$p(z) * (1+e^z) = e^z$

$p(z) = (1-p(z)) * e^z$

$e^z = p(z) / 1-p(z)$

$ln(e^z) = ln(p(z) / (1-p(z))$

$z(p) = ln(p/(1-p))$

####Part B:

$\beta_1$ measures the change in log odds comparing class label 0 to 1

Assuming $\beta_1$ is negative, $p$ goes to zero as $x_1$ goes to $\infty$ and $p$ goes to 1 as $x_1$ goes to $-\infty$

###Number 8: Logistic regression to perform classification

The logisitc regression method had the lowest misclassification error on the test set at 0.081

```{r}
glm.fit = glm(spam.train$y~. , data=spam.train, family="binomial")

log.train = spam.train[,1:(ncol(spam.train)-1)]
log.test = spam.test[,1:(ncol(spam.test)-1)]

predict_train = predict(glm.fit,log.train , type="response") 

logit_train = rep("good", nrow(spam.train)) 
logit_train[predict_train>0.5]="spam"

logit_train_error = 1 - sum(logit_train == spam.train$y)/nrow(spam.train)

predict_test = predict(glm.fit, log.test, type="response")
logit_test = rep("good", nrow(spam.test)) 
logit_test[predict_test>0.5]="spam"

logit_test_error = 1- sum(logit_test == spam.test$y)/nrow(spam.test)

records[3,1] = logit_train_error 
records[3,2] = logit_test_error
records
```

###Number 9: ROC curve

The logistic regression classification yields the highest AUC 

```{r}
library(data.table)
library(ROCR)

# Tree ROC
treetab = predict(spamtreepruned, spam.test, type="vector") 
treepred = prediction(data.table(treetab)[,spam], spam.test$y) 
treeperf = performance(treepred,measure="tpr",x.measure="fpr")

# Logi ROC
logittab = predict(glm.fit, spam.test, type="response")
logitpred = prediction(logittab, spam.test$y)
logitperf = performance(logitpred,measure="tpr",x.measure="fpr")

# Plot ROC
plot(treeperf, lwd=2,col="blue", main = "ROC") 
plot(logitperf, lwd=2,col="red", add=T)

# Calculate Tree AUC
treeperf.auc = performance(treepred,measure="auc")@y.values 
treeperf.auc
 
# Calculate Logit AUC
logitperf.auc = performance(logitpred,measure="auc")@y.values 
logitperf.auc
```

###Number 10: 

In the SPAM example, take “positive” to mean “spam”. If you are the designer of a spam filter, are you more concerned about the potential for false positive rates that are too large or true positive rates that are too small? Argue your case

As a designer I would be more concerned with the true positive rates being too small as they decrease the ROC ratio.




